#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2018/11/5 10:31
# @Author  : David
# @email   : mingren4792@126.com
# @File    : demo.py

import gensim
import re
import pandas as pd
import random as rd

from stanford_parser import stanford_tokenizer
from config import LOGOGRAM
from collections import defaultdict


def vector_demo():
    w2vec_file = 'F:/vector/glove_model.txt'
    model = gensim.models.KeyedVectors.load_word2vec_format(w2vec_file, binary=False)
    print(model["Yup"])


def logogram_processing(review):
    string = review.replace('â€™', '\'')
    string = string.replace('Iam ', ' I am').replace(' iam ', ' i am').replace(' dnt ', ' do not ')
    string = string.replace('I ve ', ' I have ').replace('I m ', ' I\'am ').replace('i m ', ' i\'m ')
    string = string.replace('Iam ', ' I am ').replace('iam ', ' i am ')
    string = string.replace('dont ', ' do not ').replace('google.co.in ', 'google').replace('hve ', ' have ')
    string = string.replace(' F ', ' Fuck ').replace('Ain\'t ', ' can\'t ').replace(' lv ', ' love ')
    string = string.replace(' ok~~ay~~ ', ' okay ').replace(' Its ', ' It is ').replace(' its ', ' it is ')
    string = string.replace('  Nd  ', ' and ').replace(' nd ', ' and ')
    string = string.replace('Thnx ', 'Than')
    review_list = string.split(' ')
    for index in range(len(review_list)):
        if review_list[index] in LOGOGRAM.keys():
            review_list[index] = LOGOGRAM[review_list[index]]
    print(' '.join(review_list))

    return ' '.join(review_list)



def divided_train_data():
    data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    test = pd.read_csv('./data/version_1/dev_merge.csv', sep='\t')

    train_data = []
    dev_data = []
    test_data = []
    for row in range(24000):
        line = []
        print('train:%d', row)
        line.append(logogram_processing(data['review'][row]))
        line.append(data['label'][row])
        train_data.append(line)

    for row in range(24000,len(data)):
        line = []
        print('train:%d', row)
        line.append(logogram_processing(data['review'][row]))
        line.append(data['label'][row])
        dev_data.append(line)

    for row in range(len(test)):
        line = []
        print('test:%d', row)
        line.append(logogram_processing(data['review'][row]))
        line.append('others')
        test_data.append(line)
    train_data_result = pd.DataFrame(data=train_data)
    train_data_result.to_csv('./data/bert_version/train.tsv', sep='\t', index=None, header=None)

    dev_data_result = pd.DataFrame(data=dev_data)
    dev_data_result.to_csv('./data/bert_version/dev.tsv', sep='\t', index=None, header=None)

    dev_data_result = pd.DataFrame(data=test_data)
    dev_data_result.to_csv('./data/bert_version/test.tsv', sep='\t', index=None, header=None)


def get_max_len():
    max = 0
    data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    for row in range(len(data)):
        if max < len(data['review'][row].split()):
            max = len(data['review'][row].split())
    print(max)


def segment(text):
    from ekphrasis.classes.tokenizer import SocialTokenizer
    social_tokenizer = SocialTokenizer(lowercase=True).tokenize
    return ' '.join(social_tokenizer(text))


def data_process():
    data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    result = []
    for row in range(1000):
        line = []
        text = data['review'][row]
        line.append(data['label'][row])
        line.append(' '.join(stanford_tokenizer(text)))
        result.append(line)

    df = pd.DataFrame(result, columns=['label', 'review'])
    df.to_csv('./data/version_2_ekphtasis/demo.csv', sep='\t', index=False)


def statistic_emotext():
    vocab = defaultdict(float)
    data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    for row in range(len(data)):
        vocab[data['label'][row]] += 1

    print(vocab)


def augment_data():
    data = pd.read_csv('./data/wassa/wassa_bert.csv', sep='\t')
    emotext_data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    angry = 0
    sad = 0
    happy = 0
    result = []

    for line_num in range(len(emotext_data)):
        line = []
        line.append(emotext_data['label'][line_num])
        line.append(emotext_data['review'][line_num])
        result.append(line)

    for row in range(len(data)):
        if angry == 9500:
            break
        line = []
        if data['label'][row] == 'angry':
            angry += 1
            line.append(data['label'][row])
            line.append(data['review'][row])
            result.append(line)

    for row in range(len(data)):
        if sad == 9600:
            break
        line = []
        if data['label'][row] == 'sad':
            sad += 1
            line.append(data['label'][row])
            line.append(data['review'][row])
            result.append(line)


    for row in range(len(data)):
        if happy == 10800:
            break
        line = []
        if data['label'][row] == 'happy':
            happy += 1
            line.append(data['label'][row])
            line.append(data['review'][row])
            result.append(line)


    df = pd.DataFrame(result, columns=['label', 'review']).sample(frac=1).reset_index(drop=True)
    df.to_csv('./data/version_2_augment/augment.csv', sep='\t', index=False)


def downsampling():
    # å‡å°‘othersç±»åˆ«çš„æ•°æ®æ€»æ•°ï¼Œä»¥è¾¾åˆ°æ•°æ®å¹³è¡¡
    data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    result = []
    others = 0
    for row in range(len(data['label'])):
        line = []
        if data['label'][row] == 'others' and others <= 5000:
            others += 1
            line.append(data['label'][row])
            line.append(data['review'][row])
            result.append(line)

        elif data['label'][row] in ['sad', 'happy', 'angry']:
            line.append(data['label'][row])
            line.append(data['review'][row])
            result.append(line)
        else:
            continue
    df = pd.DataFrame(result, columns=['label', 'review'])
    df.to_csv('./data/downsampling/train.csv', sep='\t', index=False)


def upsampling():
    # è¿‡é‡‡æ ·ï¼›å°†æ•°æ®å°‘çš„éƒ¨åˆ†è¿›è¡Œæ‹·è´ä»¥è¾¾åˆ°æ•°æ®å¹³è¡¡çš„ç›®çš„
    result = []
    data = pd.read_csv('./data/version_1/train_merge.csv', sep='\t')
    for row in range(len(data)):
        line = []
        if data['label'][row] == 'others':
            line.append(data['review'][row])
            line.append(data['label'][row])
            result.append(line)
        else:
            for i in range(3):
                line = []
                line.append(data['review'][row])
                line.append(data['label'][row])
                result.append(line)
    df = pd.DataFrame(result, columns=['review', 'label']).sample(frac=1).reset_index(drop=True)
    df.to_csv('./data/upsampling/upsampling_augment.csv', index=False, sep='\t')


def split_augment():
    data = pd.read_csv('./data/downsampling/train.csv', sep='\t')
    train_result = []
    dev_result = []
    for row in range(len(data)):
        line = []
        string = data['review'][row]
        string = clean_str(string)
        line.append(data['label'][row])
        line.append(string)
        if rd.random() < 0.8:
            train_result.append(line)
        else:
            dev_result.append(line)

    df = pd.DataFrame(train_result, columns=['label', 'review']).sample(frac=1).reset_index(drop=True)
    df.to_csv('./data/downsampling/train.tsv', index=False, sep='\t')

    df = pd.DataFrame(dev_result, columns=['label', 'review']).sample(frac=1).reset_index(drop=True)
    df.to_csv('./data/downsampling/dev.tsv', index=False, sep='\t')




def clean_str(string):
    """
        Tokenization/string cleaning for dataset
        Every dataset is lower cased except
        """
    string = string.replace(':)', ' smile ').replace(':-)', ' smile ') \
        .replace(':D', ' smile ').replace('=)', ' smile ').replace('ðŸ˜„', ' smile ').replace('â˜º', ' smile ')
    string = string.replace('â¤', ' like ').replace('<3', ' like ').replace('ðŸ’•', ' like ').replace('ðŸ˜', ' like ')
    string = string.replace('ðŸ¤—', ' happy ').replace(':-)', ' happy ')
    string = string.replace(':(', ' unhappy ').replace(':-(', ' unhappy ').replace('ðŸ’”', ' unhappy ') \
        .replace('ðŸ˜•', 'unhappy ').replace('ðŸ˜¤', ' unhappy ')
    string = string.replace('ðŸ˜¡', ' anger ').replace('ðŸ™ƒ', ' anger ')
    string = string.replace('ðŸ˜ž', ' sadness ').replace('ðŸ˜“', ' sadness ').replace('ðŸ˜”', ' sadness ')
    string = string.replace(';-;', ' unhappy ')

    string = string.replace('â€™', '\'').replace('"',' ')
    string = string.replace('whats ', 'what is')
    string = string.replace('Iam ', 'I am').replace(' iam ', 'i am').replace(' dnt ', ' do not ')
    string = string.replace('I ve ', 'I have ').replace('I m ', 'I\'am ').replace('i m ', 'i\'m ')
    string = string.replace('Iam ', 'I am ').replace('iam ', 'i am ')
    string = string.replace('dont ', 'do not ').replace('google.co.in ', 'google').replace('hve ', 'have ')
    string = string.replace(' F ', ' Fuck ').replace('Ain\'t ', ' are not ').replace(' lv ', ' love ')
    string = string.replace(' ok~~ay~~ ', ' okay ').replace(' Its ', 'It is').replace(' its ', ' it is ')
    string = string.replace('  Nd  ', ' and ').replace(' nd ', ' and ')
    string = string.replace('Thnx ', ' Thanx ').replace('[#TRIGGERWORD#]', '')

    # delete ascll
    string = re.sub('[^\x00-\x7f]', ' ', string)
    word1_list = string.split()
    for index in range(len(word1_list)):
        if word1_list[index] in LOGOGRAM.keys():
            word1_list[index] = LOGOGRAM[word1_list[index]]
    string = ' '.join(word1_list)
    # letters only
    # string = re.sub("[^a-zA-Z\'.!?]", " ", string)
    string = string.lower()
    word_list = string.split()
    for index in range(len(word_list)):
        if word_list[index] in LOGOGRAM.keys():
            word_list[index] = LOGOGRAM[word_list[index]]

    string = " ".join(word_list)

    # words = stanford_tokenizer(string)

    # stops = set(stopwords.words("english"))
    # meaningful_words = [w for w in words if not w in stops]
    # string = " ".join(words)
    return string


split_augment()
